{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset into a pandas DataFrame\n",
    "df = pd.read_csv('your_dataset.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical feature analysis "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between features with heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "corr = df.corr()\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze features scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select the features you want to analyze\n",
    "features = df[['feature1', 'feature2', 'feature3']]\n",
    "\n",
    "# Compute the mean and standard deviation of each feature\n",
    "mean = features.mean()\n",
    "std = features.std()\n",
    "\n",
    "# Print the mean and standard deviation of each feature\n",
    "print('Mean:')\n",
    "print(mean)\n",
    "print('Standard Deviation:')\n",
    "print(std)\n",
    "\n",
    "# Scale the features using StandardScaler from scikit-learn\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Convert the scaled features back to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "\n",
    "# Compute the mean and standard deviation of the scaled features\n",
    "scaled_mean = scaled_df.mean()\n",
    "scaled_std = scaled_df.std()\n",
    "\n",
    "# Print the mean and standard deviation of the scaled features\n",
    "print('Scaled Mean:')\n",
    "print(scaled_mean)\n",
    "print('Scaled Standard Deviation:')\n",
    "print(scaled_std)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze features importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini importance for trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load your dataset and split into features and target variable\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Initialize a Random Forest Classifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Fit the model to your data\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get the feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "sorted_idx = importances.argsort()[::-1]\n",
    "\n",
    "# Print the feature importances\n",
    "for i in sorted_idx:\n",
    "    print(f'Feature {X.columns[i]}: {importances[i]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Load your dataset and split into features and target variable\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Compute permutation feature importances\n",
    "result = permutation_importance(rf, X, y, n_repeats=10, random_state=0)\n",
    "\n",
    "# Get the feature importances\n",
    "importances = result.importances_mean\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "sorted_idx = importances.argsort()[::-1]\n",
    "\n",
    "# Print the feature importances\n",
    "for i in sorted_idx:\n",
    "    print(f'Feature {X.columns[i]}: {importances[i]}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze features generalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze with bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset and split into features and target variable\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Define the model you want to analyze\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Compute the learning curves\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5, train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "\n",
    "# Compute the mean and standard deviation of train and test scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training Score')\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Validation Score')\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1, color='r')\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color='g')\n",
    "plt.xlabel('Number of Training Samples')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Learning Curves')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze features outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Z-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Z-score for each feature\n",
    "z_scores = (df - df.mean()) / df.std()\n",
    "\n",
    "# Find the outliers based on Z-score threshold\n",
    "outliers = df[z_scores.abs() > 3].dropna()\n",
    "\n",
    "# Print the outliers\n",
    "print(outliers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Specify the features you want to analyze\n",
    "X = df[['feature1', 'feature2', 'feature3']]\n",
    "\n",
    "# Initialize and fit DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan.fit(X)\n",
    "\n",
    "# Get the labels assigned to each data point\n",
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data miss-balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Extract the feature matrix X and the target vector y\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Count the occurrences of each class in the target vector\n",
    "class_counts = y.value_counts()\n",
    "\n",
    "# Print the class counts\n",
    "print('Class Counts:')\n",
    "print(class_counts)\n",
    "\n",
    "# Compute the class imbalance ratio\n",
    "imbalance_ratio = class_counts.min() / class_counts.max()\n",
    "\n",
    "# Print the class imbalance ratio\n",
    "print('Class Imbalance Ratio:')\n",
    "print(imbalance_ratio)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Upsample the minority class to balance the training set\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_minority = df_train[df_train['target'] == minority_class]  # Replace minority_class with the actual minority class label\n",
    "df_majority = df_train[df_train['target'] == majority_class]  # Replace majority_class with the actual majority class label\n",
    "df_minority_upsampled = resample(df_minority, replace=True, n_samples=len(df_majority), random_state=42)\n",
    "df_train_balanced = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# Count the occurrences of each class in the balanced training set\n",
    "class_counts_balanced = df_train_balanced['target'].value_counts()\n",
    "\n",
    "# Print the class counts in the balanced training set\n",
    "print('Class Counts (Balanced Training Set):')\n",
    "print(class_counts_balanced)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
