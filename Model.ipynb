{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/sajjjadayobi/PersianQA\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/tasks/document_question_answering\n",
    "\n",
    "https://www.google.com/search?q=question+and+answer+task+rnn+pytorch&oq=question+and+answer+task+rnn+pytorch&aqs=chrome..69i57.182j0j1&sourceid=chrome&ie=UTF-8\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/chatbot_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3333333333333333\n",
      "BLEU Score: 0.43884190960972586\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def calculate_accuracy(predictions, ground_truth):\n",
    "    correct_count = 0\n",
    "    total_count = len(predictions)\n",
    "    \n",
    "    for pred, gt in zip(predictions, ground_truth):\n",
    "        if pred == gt:\n",
    "            correct_count += 1\n",
    "    \n",
    "    accuracy = correct_count / total_count\n",
    "    return accuracy\n",
    "\n",
    "def calculate_bleu_score(predictions, ground_truth):\n",
    "    # Convert predictions and ground truth to list of tokens\n",
    "    predictions = [prediction.split() for prediction in predictions]\n",
    "    ground_truth = [[answer.split()] for answer in ground_truth]\n",
    "    \n",
    "    # Calculate BLEU score\n",
    "    bleu_score = corpus_bleu(ground_truth, predictions)\n",
    "    return bleu_score\n",
    "\n",
    "# Example usage\n",
    "predictions = [\"The answer is A.\", \"I think it's B.\", \"The correct answer is C.\"]\n",
    "ground_truth = [\"The answer is A.\", \"The correct answer is B.\", \"The answer is C.\"]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(predictions, ground_truth)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = calculate_bleu_score(predictions, ground_truth)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/Load/QA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import word_tokenize, Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "tokenized_texts = [word_tokenize(normalizer.normalize(text)) for text in pd.concat([df['question'], df['answer']])]\n",
    "\n",
    "vocab_index = {'' : 0}\n",
    "index_vocab = {0 : ''}\n",
    "index = 1\n",
    "\n",
    "for text_tokens in tokenized_texts:\n",
    "    for token in text_tokens:\n",
    "        token = normalizer.normalize(token)\n",
    "        if token not in vocab_index:\n",
    "            vocab_index[token] = index\n",
    "            index_vocab[index] = token\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = len(max(tokenized_texts, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['answer_token'] = [[vocab_index[token] for token in word_tokenize(normalizer.normalize(text))] for text in df['answer']]\n",
    "df['question_token'] = [[vocab_index[token] for token in word_tokenize(normalizer.normalize(text))] for text in df['question']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len question: 484\n",
      "max len answer: 2020\n"
     ]
    }
   ],
   "source": [
    "print(\"max len question:\", df['question_token'].str.len().max())\n",
    "print(\"max len answer:\", df['answer_token'].str.len().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " احتیاط ترک تقلید ابتدایی میت است اما بقای بر تقلید میت در مسائلی که عمل کرده یا اخذ فتوا برای عمل نموده جایز است.\n",
      "احتیاط ترک تقلید ابتدایی میت است اما بقای بر تقلید میت در مسائلی که عمل کرده یا اخذ فتوا برای عمل نموده جایز است .\n"
     ]
    }
   ],
   "source": [
    "print(df['answer'][1])\n",
    "output_tokens = [index_vocab[index] for index in df['answer_token'][1]]\n",
    "print(\" \".join(output_tokens))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# Step 4: Padding\n",
    "padded_questions = pad_sequence([torch.tensor(encoded) for encoded in df['question_token']], batch_first=True, padding_value=0)\n",
    "padded_answers = pad_sequence([torch.tensor(encoded) for encoded in df['answer_token']], batch_first=True, padding_value=0)\n",
    "\n",
    "# Pad the sequences to the maximum length\n",
    "padded_questions = torch.nn.functional.pad(padded_questions, (0, max_length - padded_questions.size(1)))\n",
    "padded_answers = torch.nn.functional.pad(padded_answers, (0, max_length - padded_answers.size(1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 1914\n",
      "Test data size: 479\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train-test split\n",
    "train_questions, test_questions, train_answers, test_answers = train_test_split(\n",
    "    padded_questions, padded_answers, test_size=0.2\n",
    ")\n",
    "\n",
    "# Print some information\n",
    "print(\"Train data size:\", len(train_questions))\n",
    "print(\"Test data size:\", len(test_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        hidden = torch.zeros(1, self.hidden_size).to(torch.float32)\n",
    "        input_seq = input_seq.to(torch.float32)\n",
    "        output, _ = self.rnn(input_seq, hidden)\n",
    "        output = self.fc(output[0])  # Select the last time step's output\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8878,   52, 4533,  ...,    0,    0,    0])\n",
      "tensor([ 0.2720,  0.6777,  0.6288,  ..., -1.0869, -0.6478,  0.4955],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(label)\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(output)\n\u001b[0;32m---> 19\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, label)\n\u001b[1;32m     20\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Disks/Learn/All/Learn/mlops/projects/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Disks/Learn/All/Learn/mlops/projects/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/Disks/Learn/All/Learn/mlops/projects/.venv/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "input_size = 2000\n",
    "hidden_size = 1000\n",
    "output_size = 2000\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5\n",
    "\n",
    "model = RNNModel(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for input, label in zip(train_questions, train_answers):\n",
    "        optimizer.zero_grad()\n",
    "        input = input.unsqueeze(0)\n",
    "        output = model(input)\n",
    "        print(label)\n",
    "        print(output)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for input, label in zip(test_questions, test_answers):\n",
    "        output = model(input)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print('Test Accuracy: {:.2f}%'.format(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4676,  0.0028, -0.7883,  ..., -0.4431, -0.5125, -0.0923],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
